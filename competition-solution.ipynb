{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb34e7bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:05:54.505962Z",
     "iopub.status.busy": "2025-03-11T00:05:54.505644Z",
     "iopub.status.idle": "2025-03-11T00:05:54.509750Z",
     "shell.execute_reply": "2025-03-11T00:05:54.509137Z"
    },
    "papermill": {
     "duration": 0.012919,
     "end_time": "2025-03-11T00:05:54.510878",
     "exception": false,
     "start_time": "2025-03-11T00:05:54.497959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/discussion/560682#3113134\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a47cea1",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-03-11T00:05:54.523838Z",
     "iopub.status.busy": "2025-03-11T00:05:54.523591Z",
     "iopub.status.idle": "2025-03-11T00:06:07.715710Z",
     "shell.execute_reply": "2025-03-11T00:06:07.715014Z"
    },
    "papermill": {
     "duration": 13.200053,
     "end_time": "2025-03-11T00:06:07.717271",
     "exception": false,
     "start_time": "2025-03-11T00:05:54.517218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import kaggle_evaluation.konwinski_prize_inference_server\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "start_time = time.time()\n",
    "allowed_time = [start_time + 60 * 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c8fb7",
   "metadata": {
    "papermill": {
     "duration": 0.005949,
     "end_time": "2025-03-11T00:06:07.729592",
     "exception": false,
     "start_time": "2025-03-11T00:06:07.723643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The evaluation API requires that you set up a server which will respond to inference requests. We have already defined the server; you just need write the predict function. When we evaluate your submission on the hidden test set the client defined in `konwinski_prize_gateway` will run in a different container with direct access to the hidden test set and hand off the data.\n",
    "\n",
    "Your code will always have access to the published copies of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4448de25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:06:07.742742Z",
     "iopub.status.busy": "2025-03-11T00:06:07.742309Z",
     "iopub.status.idle": "2025-03-11T00:06:07.746199Z",
     "shell.execute_reply": "2025-03-11T00:06:07.745582Z"
    },
    "papermill": {
     "duration": 0.011473,
     "end_time": "2025-03-11T00:06:07.747195",
     "exception": false,
     "start_time": "2025-03-11T00:06:07.735722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_count: Optional[int] = None\n",
    "\n",
    "\n",
    "def get_number_of_instances(num_instances: int) -> None:\n",
    "    \"\"\"The very first message from the gateway will be the total number of instances to be served.\n",
    "    You don't need to edit this function.\n",
    "    \"\"\"\n",
    "    global instance_count\n",
    "    instance_count = num_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e43e427",
   "metadata": {
    "papermill": {
     "duration": 0.005798,
     "end_time": "2025-03-11T00:06:07.759039",
     "exception": false,
     "start_time": "2025-03-11T00:06:07.753241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e4bd091",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-03-11T00:06:07.771806Z",
     "iopub.status.busy": "2025-03-11T00:06:07.771529Z",
     "iopub.status.idle": "2025-03-11T00:11:44.160635Z",
     "shell.execute_reply": "2025-03-11T00:11:44.159821Z"
    },
    "papermill": {
     "duration": 336.397064,
     "end_time": "2025-03-11T00:11:44.162096",
     "exception": false,
     "start_time": "2025-03-11T00:06:07.765032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 00:07:17 __init__.py:183] Automatically detected platform cuda.\n",
      "INFO 03-11 00:07:56 config.py:526] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 03-11 00:08:01 awq_marlin.py:109] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 03-11 00:08:01 config.py:1383] Defaulting to use mp for distributed inference\n",
      "WARNING 03-11 00:08:01 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-11 00:08:01 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='/kaggle/input/m/huikang/deepseek-r1/transformers/qwen-qwq-32b-awq/1', speculative_config=None, tokenizer='/kaggle/input/m/huikang/deepseek-r1/transformers/qwen-qwq-32b-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=2024, served_model_name=/kaggle/input/m/huikang/deepseek-r1/transformers/qwen-qwq-32b-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[8,4,2,1],\"max_capture_size\":8}, use_cached_outputs=False, \n",
      "WARNING 03-11 00:08:01 multiproc_worker_utils.py:298] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 03-11 00:08:01 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 00:08:01 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m INFO 03-11 00:08:01 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m INFO 03-11 00:08:01 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "WARNING 03-11 00:08:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m WARNING 03-11 00:08:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 00:08:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 00:08:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-11 00:08:02 cuda.py:235] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 00:08:02 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 03-11 00:08:02 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 03-11 00:08:02 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 03-11 00:08:13 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 03-11 00:08:13 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m INFO 03-11 00:08:13 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 03-11 00:08:13 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m INFO 03-11 00:08:13 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "INFO 03-11 00:08:13 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 00:08:13 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 00:08:13 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "WARNING 03-11 00:08:14 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 03-11 00:08:14 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 03-11 00:08:14 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 03-11 00:08:14 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 03-11 00:08:14 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_13fb8d26'), local_subscribe_port=58765, remote_subscribe_port=None)\n",
      "INFO 03-11 00:08:14 model_runner.py:1111] Starting to load model /kaggle/input/m/huikang/deepseek-r1/transformers/qwen-qwq-32b-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m INFO 03-11 00:08:14 model_runner.py:1111] Starting to load model /kaggle/input/m/huikang/deepseek-r1/transformers/qwen-qwq-32b-awq/1...\n",
      "INFO 03-11 00:08:14 model_runner.py:1111] Starting to load model /kaggle/input/m/huikang/deepseek-r1/transformers/qwen-qwq-32b-awq/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 00:08:14 model_runner.py:1111] Starting to load model /kaggle/input/m/huikang/deepseek-r1/transformers/qwen-qwq-32b-awq/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f311b5e5514069857025940bef314a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m INFO 03-11 00:10:54 model_runner.py:1116] Loading model weights took 4.5673 GB\n",
      "INFO 03-11 00:10:54 model_runner.py:1116] Loading model weights took 4.5673 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 00:10:54 model_runner.py:1116] Loading model weights took 4.5673 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m INFO 03-11 00:10:54 model_runner.py:1116] Loading model weights took 4.5673 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m INFO 03-11 00:11:35 worker.py:266] Memory profiling takes 39.82 seconds\r\n",
      "WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 00:11:35 worker.py:266] Memory profiling takes 39.82 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m INFO 03-11 00:11:35 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m INFO 03-11 00:11:35 worker.py:266] Memory profiling takes 39.82 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-11 00:11:35 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m INFO 03-11 00:11:35 worker.py:266] model weights take 4.57GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.96GiB; the rest of the memory reserved for KV Cache is 13.47GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m INFO 03-11 00:11:35 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "INFO 03-11 00:11:35 worker.py:266] model weights take 4.57GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.96GiB; the rest of the memory reserved for KV Cache is 13.47GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m INFO 03-11 00:11:35 worker.py:266] model weights take 4.57GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.96GiB; the rest of the memory reserved for KV Cache is 13.47GiB.\n",
      "WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-11 00:11:35 worker.py:266] Memory profiling takes 40.25 seconds\r\n",
      "INFO 03-11 00:11:35 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "INFO 03-11 00:11:35 worker.py:266] model weights take 4.57GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.96GiB; the rest of the memory reserved for KV Cache is 13.47GiB.\n",
      "INFO 03-11 00:11:35 executor_base.py:108] # CUDA blocks: 13794, # CPU blocks: 4096\n",
      "INFO 03-11 00:11:35 executor_base.py:113] Maximum concurrency for 32768 tokens per request: 6.74x\n",
      "WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 03-11 00:11:35 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-11 00:11:38 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m INFO 03-11 00:11:38 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-11 00:11:38 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-11 00:11:38 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 4/4 [00:04<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 00:11:43 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.08 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=367)\u001b[0;0m INFO 03-11 00:11:43 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.08 GiB\n",
      "INFO 03-11 00:11:43 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.08 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=362)\u001b[0;0m INFO 03-11 00:11:43 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.08 GiB\n",
      "INFO 03-11 00:11:43 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 49.11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams, RequestOutput\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") or os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    llm_model_pth: str = (\n",
    "        \"/kaggle/input/m/huikang/deepseek-r1/transformers/qwen-qwq-32b-awq/1\"\n",
    "    )\n",
    "else:\n",
    "    llm_model_pth: str = \"/root/volume/KirillR/QwQ-32B-Preview-AWQ\"\n",
    "\n",
    "BATCH_SIZE: int = 7\n",
    "VALIDATION_COPY_COUNT: int = 3\n",
    "MAX_TOKENS: int = 4096\n",
    "\n",
    "MAX_NUM_SEQS: int = 7\n",
    "MAX_MODEL_LEN: int = 32_768\n",
    "\n",
    "llm: LLM = LLM(\n",
    "    llm_model_pth,\n",
    "    dtype=\"float16\",\n",
    "    max_num_seqs=MAX_NUM_SEQS,  # Maximum number of sequences per iteration. Default is 256\n",
    "    max_model_len=MAX_MODEL_LEN,  # Model context length\n",
    "    trust_remote_code=True,  # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n",
    "    tensor_parallel_size=4,  # The number of GPUs to use for distributed execution with tensor parallelism\n",
    "    gpu_memory_utilization=0.95,  # The ratio (between 0 and 1) of GPU memory to reserve for the model\n",
    "    seed=2024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f1f0977",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:44.188774Z",
     "iopub.status.busy": "2025-03-11T00:11:44.188477Z",
     "iopub.status.idle": "2025-03-11T00:11:44.192011Z",
     "shell.execute_reply": "2025-03-11T00:11:44.191360Z"
    },
    "papermill": {
     "duration": 0.017943,
     "end_time": "2025-03-11T00:11:44.193076",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.175133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b485a",
   "metadata": {
    "papermill": {
     "duration": 0.012383,
     "end_time": "2025-03-11T00:11:44.218078",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.205695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "947a073f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:44.243630Z",
     "iopub.status.busy": "2025-03-11T00:11:44.243374Z",
     "iopub.status.idle": "2025-03-11T00:11:44.247161Z",
     "shell.execute_reply": "2025-03-11T00:11:44.246528Z"
    },
    "papermill": {
     "duration": 0.017701,
     "end_time": "2025-03-11T00:11:44.248216",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.230515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def stringify_directory(directory: str) -> str:\n",
    "    full_paths: List[str] = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            full_path: str = os.path.join(root, file)\n",
    "            full_paths.append(full_path)\n",
    "    return \"\\n\".join(full_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "942d1883",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:44.273641Z",
     "iopub.status.busy": "2025-03-11T00:11:44.273388Z",
     "iopub.status.idle": "2025-03-11T00:11:44.279320Z",
     "shell.execute_reply": "2025-03-11T00:11:44.278677Z"
    },
    "papermill": {
     "duration": 0.019779,
     "end_time": "2025-03-11T00:11:44.280389",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.260610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_file_query(xml_content: str) -> Dict[str, List[str]]:\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    # Prepare a data structure to collect results\n",
    "    parsed_data: Dict[str, List[str]] = {}\n",
    "    pattern: str = r\"<root>(.*?)</root>\"\n",
    "    matches: List[str] = re.findall(pattern, xml_content, re.DOTALL)\n",
    "\n",
    "    for match in matches:\n",
    "        try:\n",
    "            # Parse the XML\n",
    "            root = ET.fromstring(\"<root>\" + match + \"</root>\")\n",
    "\n",
    "            # Find all <entry> elements\n",
    "            for entry in root.findall(\"entry\"):\n",
    "                # Extract the <filepath> text\n",
    "                filepath = entry.find(\"filepath\")\n",
    "                filepath_text: Optional[str] = (\n",
    "                    filepath.text.strip()\n",
    "                    if filepath is not None and filepath.text is not None\n",
    "                    else None\n",
    "                )\n",
    "\n",
    "                # Locate <strings_to_search> container\n",
    "                strings_container = entry.find(\"strings_to_search\")\n",
    "\n",
    "                # Gather each <string_to_search> text\n",
    "                search_strings: List[str] = []\n",
    "                if strings_container is not None:\n",
    "                    for s in strings_container.findall(\"string_to_search\"):\n",
    "                        if s.text is not None:\n",
    "                            search_strings.append(s.text.strip())\n",
    "\n",
    "                # Store in a dictionary: { filepath: [search_strings...] }\n",
    "                parsed_data[filepath_text] = search_strings  # type: ignore\n",
    "        except:\n",
    "            print(\"Error parsing output\")\n",
    "            print(xml_content)\n",
    "            return {}\n",
    "\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6245a53b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:44.305964Z",
     "iopub.status.busy": "2025-03-11T00:11:44.305686Z",
     "iopub.status.idle": "2025-03-11T00:11:44.312907Z",
     "shell.execute_reply": "2025-03-11T00:11:44.312287Z"
    },
    "papermill": {
     "duration": 0.021101,
     "end_time": "2025-03-11T00:11:44.313910",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.292809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reading_prompt: str = (\n",
    "    \"\"\"\n",
    "You will be implementing a git diff patch to solve an issue with the code repository.\n",
    "You will first need to select files in the file directory.\n",
    "\n",
    "This is the problem statement.\n",
    "\n",
    "{problem_statement}\n",
    "\n",
    "This is the file directory\n",
    "\n",
    "<directory>\n",
    "{directory_string}\n",
    "</directory>\n",
    "\n",
    "Which files should be inspected so that we can solve the problem?\n",
    "When we inspect each file, what strings should be searched?\n",
    "\n",
    "Return the strings to search in this format\n",
    "\n",
    "(explanation)\n",
    "\n",
    "<root>\n",
    "    <entry>\n",
    "        <filepath>filepath</filepath>  \n",
    "        <strings_to_search>\n",
    "            <string_to_search>string_to_search</string_to_search>\n",
    "            ...\n",
    "            <string_to_search>string_to_search</string_to_search>\n",
    "        </strings_to_search>\n",
    "    </entry>\n",
    "    <entry>\n",
    "        <filepath>filepath</filepath>\n",
    "        <strings_to_search>\n",
    "            <string_to_search>string_to_search</string_to_search>\n",
    "            ...\n",
    "            <string_to_search>string_to_search</string_to_search>\n",
    "        </strings_to_search>\n",
    "    </entry>\n",
    "    ...\n",
    "</root>\n",
    "...\n",
    "\n",
    "Notes:\n",
    "- Make sure to encode each entry between <root> and </root>\n",
    "- Return the FULL filepath - exactly as specified in <directory> and </directory>\n",
    "    - Example: <filepath>repo/path/to/directory/file.py</filepath>\n",
    "- If you are searching for a word instead of a substring, maybe add spaces or brackets before and after the string\n",
    "    - For example, if you are searching for uses of the function `calculate`, use ` calculate(` as the search string instead of `calculate`\n",
    "- Prefer searching longer strings\n",
    "    - Avoid searching for strings that might appear in many parts of the codebase\n",
    "- Search the test files as well to understand the feature behavior\n",
    "    - Also search for the relevant function calls in the test files\n",
    "\"\"\".strip()\n",
    ")\n",
    "\n",
    "\n",
    "def get_selection_query(\n",
    "    directory_string: str, problem_statement: str\n",
    ") -> Tuple[List[str], List[Dict[str, List[str]]]]:\n",
    "    sampling_params: SamplingParams = SamplingParams(\n",
    "        temperature=0.6,  # randomness of the sampling\n",
    "        min_p=0.01,\n",
    "        skip_special_tokens=True,  # Whether to skip special tokens in the output\n",
    "        max_tokens=MAX_TOKENS,\n",
    "    )\n",
    "\n",
    "    list_of_messages: List[List[Dict[str, str]]] = [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": reading_prompt.format(\n",
    "                    problem_statement=problem_statement[:20_000],\n",
    "                    directory_string=directory_string[:30_000],\n",
    "                ),\n",
    "            },\n",
    "        ]\n",
    "        for _ in range(BATCH_SIZE)\n",
    "    ]\n",
    "\n",
    "    prompt_texts: List[str] = [\n",
    "        (\n",
    "            tokenizer.apply_chat_template(\n",
    "                conversation=messages, tokenize=False, add_generation_prompt=True\n",
    "            )  # type: ignore\n",
    "        )\n",
    "        + \"<think>\\n\"\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "    # print(prompt_texts)\n",
    "\n",
    "    print(\"get_selection_query\", [count_tokens(text) for text in prompt_texts])\n",
    "    request_outputs: list[RequestOutput] = llm.generate(\n",
    "        prompt_texts, sampling_params=sampling_params\n",
    "    )\n",
    "    if not request_outputs:\n",
    "        return [], []\n",
    "    response_texts: List[str] = [\n",
    "        request_output.outputs[0].text for request_output in request_outputs\n",
    "    ]\n",
    "    print(\"get_selection_query\", [count_tokens(text) for text in response_texts])\n",
    "\n",
    "    completion_texts = [\n",
    "        prompt_text + response_text\n",
    "        for prompt_text, response_text in zip(prompt_texts, response_texts)\n",
    "    ]\n",
    "    file_queries: List[Dict[str, List[str]]] = [\n",
    "        extract_file_query(response_text) for response_text in response_texts\n",
    "    ]\n",
    "    return completion_texts, file_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc44093a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:44.339614Z",
     "iopub.status.busy": "2025-03-11T00:11:44.339368Z",
     "iopub.status.idle": "2025-03-11T00:11:44.344757Z",
     "shell.execute_reply": "2025-03-11T00:11:44.344172Z"
    },
    "papermill": {
     "duration": 0.019364,
     "end_time": "2025-03-11T00:11:44.345755",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.326391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup(\n",
    "    repo_archive: io.BytesIO,\n",
    "    pip_packages_archive: io.BytesIO,\n",
    "    env_setup_cmds_templates: list[str],\n",
    "    repo_path: str,\n",
    ") -> None:\n",
    "    \"\"\"Replace this function with your inference code.\n",
    "    Args:\n",
    "        problem_statement: The text of the git issue.\n",
    "        repo_path: A BytesIO buffer path with a .tar containing the codebase that must be patched. The gateway will make this directory available immediately before this function runs.\n",
    "        pip_packages_archive: A BytesIO buffer path with a .tar containing the wheel files necessary for running unit tests.\n",
    "        env_setup_cmds_templates: Commands necessary for installing the pip_packages_archive.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the codebase to be patched into a directory that won't be exported when\n",
    "    # the notebook is saved.\n",
    "    archive_path = \"/tmp/repo_archive.tar\"\n",
    "    with open(archive_path, \"wb\") as f:\n",
    "        f.write(repo_archive.read())\n",
    "    if os.path.exists(repo_path):\n",
    "        shutil.rmtree(repo_path)\n",
    "    shutil.unpack_archive(archive_path, extract_dir=repo_path)\n",
    "    os.remove(archive_path)\n",
    "\n",
    "    \"\"\"\n",
    "    Unpack pip_packages if you want to run unit tests on your patch.\n",
    "    Note that editing unit tests with your patch -- even to add valid tests -- can cause your submission to be flagged as a failure.\n",
    "    Most of the relevant repos use pytest for running tests. You will almost certainly need to run only a subset of the unit tests to avoid running out of inference time.\n",
    "    \"\"\"\n",
    "    pip_archive_dir = \"/tmp/pip_packages_archive.tar\"\n",
    "    with open(pip_archive_dir, \"wb\") as f:\n",
    "        f.write(pip_packages_archive.read())\n",
    "    pip_packages_path = \"/path/to/pip_packages\"\n",
    "    if os.path.exists(pip_packages_path):\n",
    "        shutil.rmtree(pip_packages_path)\n",
    "    shutil.unpack_archive(pip_archive_dir, extract_dir=pip_packages_path)\n",
    "    os.remove(pip_archive_dir)\n",
    "\n",
    "    # Get env setup cmds by setting the pip_packages_path\n",
    "    env_setup_cmds = [\n",
    "        cmd.format(pip_packages_path=pip_packages_path)\n",
    "        for cmd in env_setup_cmds_templates\n",
    "    ]\n",
    "\n",
    "    # Run env setup for the repo\n",
    "    subprocess.run(\n",
    "        \"\\n\".join(env_setup_cmds),\n",
    "        shell=True,\n",
    "        executable=\"/bin/bash\",\n",
    "        cwd=repo_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99fd7504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:44.371335Z",
     "iopub.status.busy": "2025-03-11T00:11:44.371095Z",
     "iopub.status.idle": "2025-03-11T00:11:44.386235Z",
     "shell.execute_reply": "2025-03-11T00:11:44.385600Z"
    },
    "papermill": {
     "duration": 0.029151,
     "end_time": "2025-03-11T00:11:44.387259",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.358108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPO_PATH: str = \"repo\"\n",
    "\n",
    "\n",
    "def fetch_file_contents(\n",
    "    files_to_search: Dict[str, List[str]], context_lines: int = 12, max_gap: int = 0\n",
    ") -> str:\n",
    "    from io import StringIO\n",
    "    from typing import Tuple\n",
    "\n",
    "    def find_lines_in_files_with_context(\n",
    "        search_map: Dict[str, List[str]], context_lines: int = context_lines\n",
    "    ) -> List[List[List[Tuple[int, str]]]]:\n",
    "        \"\"\"\n",
    "        Given a dictionary mapping file paths to a list of search terms,\n",
    "        open each file and gather *snippets* of lines that contain any\n",
    "        of those search terms, including 'context_lines' before and after.\n",
    "\n",
    "        Returns a list of lists:\n",
    "        [\n",
    "          [  # For file1\n",
    "             [ (line_number, text), (line_number, text), ... ],\n",
    "             [ ... ],\n",
    "          ],\n",
    "          [  # For file2\n",
    "             ...\n",
    "          ],\n",
    "          ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "        all_matches_per_file: List[List[List[Tuple[int, str]]]] = []\n",
    "\n",
    "        for path, terms in search_map.items():\n",
    "            if not os.path.isfile(path):\n",
    "                # If the file is not found, record an empty list\n",
    "                all_matches_per_file.append([])\n",
    "                continue\n",
    "\n",
    "            with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            file_snippets: List[List[Tuple[int, str]]] = []\n",
    "            num_lines: int = len(lines)\n",
    "\n",
    "            for i, line in enumerate(lines, start=1):\n",
    "                if any(t in line for t in terms):\n",
    "                    start_idx: int = max(1, i - context_lines)\n",
    "                    end_idx: int = min(num_lines, i + context_lines)\n",
    "                    snippet: List[Tuple[int, str]] = []\n",
    "                    for snippet_no in range(start_idx, end_idx + 1):\n",
    "                        text_content: str = lines[snippet_no - 1].rstrip(\"\\n\")\n",
    "                        snippet.append((snippet_no, text_content))\n",
    "                    file_snippets.append(snippet)\n",
    "\n",
    "            all_matches_per_file.append(file_snippets)\n",
    "\n",
    "        return all_matches_per_file\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. MERGE OVERLAPPING/ADJACENT SNIPPETS\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    def merge_file_snippets(\n",
    "        file_snippets: List[List[Tuple[int, str]]], gap: int = 0\n",
    "    ) -> List[List[Tuple[int, str]]]:\n",
    "        \"\"\"\n",
    "        Merge overlapping or nearly adjacent snippets in a single file’s snippet list.\n",
    "        \"\"\"\n",
    "        intervals: List[Tuple[int, int, List[Tuple[int, str]]]] = []\n",
    "        for snippet in file_snippets:\n",
    "            if snippet:\n",
    "                start_line: int = snippet[0][0]\n",
    "                end_line: int = snippet[-1][0]\n",
    "                intervals.append((start_line, end_line, snippet))\n",
    "\n",
    "        intervals.sort(key=lambda x: x[0])  # sort by start line\n",
    "\n",
    "        merged: List[Tuple[int, int, List[Tuple[int, str]]]] = []\n",
    "        for start, end, snippet in intervals:\n",
    "            if not merged:\n",
    "                merged.append((start, end, snippet))\n",
    "                continue\n",
    "\n",
    "            prev_start, prev_end, prev_snippet = merged[-1]\n",
    "            if start <= prev_end + gap:\n",
    "                new_end: int = max(end, prev_end)\n",
    "                combined_dict: Dict[int, str] = {}\n",
    "                for ln, txt in prev_snippet:\n",
    "                    combined_dict[ln] = txt\n",
    "                for ln, txt in snippet:\n",
    "                    combined_dict[ln] = txt\n",
    "                merged_snippet: List[Tuple[int, str]] = [\n",
    "                    (ln, combined_dict[ln]) for ln in sorted(combined_dict)\n",
    "                ]\n",
    "                merged[-1] = (prev_start, new_end, merged_snippet)\n",
    "            else:\n",
    "                merged.append((start, end, snippet))\n",
    "\n",
    "        # Extract just the merged snippet portion\n",
    "        return [x[2] for x in merged]\n",
    "\n",
    "    def merge_all_snippets(\n",
    "        all_files_snips: List[List[List[Tuple[int, str]]]], gap: int = 0\n",
    "    ) -> List[List[List[Tuple[int, str]]]]:\n",
    "        \"\"\"\n",
    "        Merge snippet blocks within each file.\n",
    "        all_files_snips is a list-of-lists:\n",
    "          [\n",
    "            [ snippetA, snippetB, ... ],  # file 1\n",
    "            [ snippetC, snippetD, ... ],  # file 2\n",
    "          ]\n",
    "        \"\"\"\n",
    "        merged: List[List[List[Tuple[int, str]]]] = []\n",
    "        for snips in all_files_snips:\n",
    "            merged.append(merge_file_snippets(snips, gap=gap))\n",
    "        return merged\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. RUN LOGIC: generate files, search, merge, and BUILD A STRING\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    has_any_matches: bool = False\n",
    "\n",
    "    # 1) Gather snippets around each match\n",
    "    context_snippets: List[List[List[Tuple[int, str]]]] = (\n",
    "        find_lines_in_files_with_context(files_to_search, context_lines=context_lines)\n",
    "    )\n",
    "\n",
    "    # 2) Merge overlapping snippets\n",
    "    merged_snips: List[List[List[Tuple[int, str]]]] = merge_all_snippets(\n",
    "        context_snippets, gap=max_gap\n",
    "    )\n",
    "\n",
    "    # 3) Build a string (instead of printing)\n",
    "    output = StringIO()\n",
    "\n",
    "    # Header\n",
    "    output.write(\"Sample files created successfully.\\n\\n\")\n",
    "    output.write(\"Search Results (by file, merging any overlapping context):\\n\\n\")\n",
    "\n",
    "    # For each file\n",
    "    for (filepath, terms), snippet_list in zip(files_to_search.items(), merged_snips):\n",
    "        output.write(f\"[file name]: {filepath[len(REPO_PATH) + 1:]}\\n\")\n",
    "        terms_searched_as_str = \"\\n\".join(terms)\n",
    "        output.write(f\"[terms searched]:\\n{terms_searched_as_str}\\n\")\n",
    "        output.write(\"[file content begin]\\n\")\n",
    "        if not snippet_list:\n",
    "            output.write(\"  No matches found.\\n\")\n",
    "        else:\n",
    "            has_any_matches = True\n",
    "            for snippet_idx, snippet in enumerate(snippet_list, start=1):\n",
    "                snippet_start: int = snippet[0][0]\n",
    "                snippet_end: int = snippet[-1][0]\n",
    "                output.write(\n",
    "                    f\"\\nMatch #{snippet_idx}, lines {snippet_start} to {snippet_end}:\\n\"\n",
    "                )\n",
    "                for line_no, text in snippet:\n",
    "                    output.write(f\"  {line_no:3d} | {text}\\n\")\n",
    "                output.write(\"\\n\")\n",
    "        output.write(\"[file content end]\\n\\n\")\n",
    "\n",
    "    file_content_string: str = output.getvalue()\n",
    "\n",
    "    if has_any_matches:\n",
    "        return file_content_string\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a50c49a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:44.412920Z",
     "iopub.status.busy": "2025-03-11T00:11:44.412648Z",
     "iopub.status.idle": "2025-03-11T00:11:44.416052Z",
     "shell.execute_reply": "2025-03-11T00:11:44.415437Z"
    },
    "papermill": {
     "duration": 0.017362,
     "end_time": "2025-03-11T00:11:44.417083",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.399721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_patch_string(text: str) -> Optional[str]:\n",
    "    pattern: str = r\"\\n```diff\\n(.*?)\\n```\"\n",
    "    matches: List[str] = re.findall(pattern, text, re.DOTALL)\n",
    "    if not matches:\n",
    "        return None\n",
    "    return matches[-1] + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56192375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:44.442809Z",
     "iopub.status.busy": "2025-03-11T00:11:44.442545Z",
     "iopub.status.idle": "2025-03-11T00:11:44.450201Z",
     "shell.execute_reply": "2025-03-11T00:11:44.449575Z"
    },
    "papermill": {
     "duration": 0.021668,
     "end_time": "2025-03-11T00:11:44.451240",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.429572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "patching_prompt: str = (\n",
    "    \"\"\"\n",
    "You will be implementing a git diff patch to solve an issue with the code repository.\n",
    "This is the problem statement.\n",
    "\n",
    "{problem_statement}\n",
    "\n",
    "These are the files that is thought to be relevant\n",
    "\n",
    "{file_content_string}\n",
    "\n",
    "Write a git diff within ```diff and ``` that fully fixes the problem.\n",
    "The git diff should not cause other tests to fail.\n",
    "\n",
    "Example:\n",
    "\n",
    "```diff\n",
    "--- a/first.txt\n",
    "+++ b/first.txt\n",
    "@@ -1,3 +1,3 @@\n",
    " start\n",
    "-first change\n",
    "+new first change\n",
    " middle\n",
    "@@ -7,4 +7,4 @@\n",
    " some content\n",
    "-second change\n",
    "+new second change\n",
    " more content\n",
    "--- a/second.txt\n",
    "+++ b/second.txt\n",
    "@@ -1,3 +1,3 @@\n",
    " beginning\n",
    "-old line\n",
    "+new line\n",
    " end\n",
    "```\n",
    "\n",
    "Reminder\n",
    "- Put your diff within ```diff and ``` and make sure the diff is valid.\n",
    "- Only the last diff printed will be considered.\n",
    "\"\"\".strip()\n",
    ")\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def get_patch_string(\n",
    "    problem_statement: str, file_content_strings: List[str]\n",
    ") -> Tuple[List[str], List[Optional[str]]]:\n",
    "    sampling_params: SamplingParams = SamplingParams(\n",
    "        temperature=0.7,  # randomness of the sampling\n",
    "        min_p=0.01,\n",
    "        skip_special_tokens=True,  # Whether to skip special tokens in the output\n",
    "        max_tokens=MAX_TOKENS,\n",
    "    )\n",
    "\n",
    "    inference_idx_to_input_idx: list[int] = [\n",
    "        input_idx\n",
    "        for input_idx, file_content_string in enumerate(file_content_strings)\n",
    "        if file_content_string != \"\"\n",
    "    ]\n",
    "\n",
    "    list_of_messages: List[List[Dict[str, str]]] = [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": patching_prompt.format(\n",
    "                    problem_statement=problem_statement[:20_000],\n",
    "                    file_content_string=file_content_strings[input_idx][:30_000],\n",
    "                ),\n",
    "            },\n",
    "        ]\n",
    "        for input_idx in inference_idx_to_input_idx\n",
    "    ]\n",
    "\n",
    "    prompt_texts: List[str] = [\n",
    "        (\n",
    "            tokenizer.apply_chat_template(\n",
    "                conversation=messages, tokenize=False, add_generation_prompt=True\n",
    "            )  # type: ignore\n",
    "        )\n",
    "        + \"<think>\\n\"\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "    # print(prompt_texts)\n",
    "\n",
    "    print(\"get_patch_string\", [count_tokens(text) for text in prompt_texts])\n",
    "    request_outputs: list[RequestOutput] = llm.generate(\n",
    "        prompt_texts, sampling_params=sampling_params\n",
    "    )\n",
    "    response_texts_from_inference: List[str] = [\n",
    "        request_output.outputs[0].text for request_output in request_outputs\n",
    "    ]\n",
    "    print(\n",
    "        \"get_patch_string\",\n",
    "        [count_tokens(text) for text in response_texts_from_inference],\n",
    "    )\n",
    "    completion_texts_from_inference = [\n",
    "        prompt_text + response_text\n",
    "        for prompt_text, response_text in zip(\n",
    "            prompt_texts, response_texts_from_inference\n",
    "        )\n",
    "    ]\n",
    "    patch_strings_from_inference: List[Optional[str]] = [\n",
    "        extract_patch_string(response_text)\n",
    "        for response_text in response_texts_from_inference\n",
    "    ]\n",
    "\n",
    "    completion_texts: list[str] = [\"\" for _ in file_content_strings]\n",
    "    patch_strings: List[Optional[str]] = [None for _ in file_content_strings]\n",
    "    for inference_idx, (completion_text, patch_string) in enumerate(\n",
    "        zip(completion_texts_from_inference, patch_strings_from_inference)\n",
    "    ):\n",
    "        input_idx = inference_idx_to_input_idx[inference_idx]\n",
    "        completion_texts[input_idx] = completion_text\n",
    "        patch_strings[input_idx] = patch_string\n",
    "\n",
    "    return completion_texts, patch_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73a5309a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:44.477001Z",
     "iopub.status.busy": "2025-03-11T00:11:44.476734Z",
     "iopub.status.idle": "2025-03-11T00:11:44.488288Z",
     "shell.execute_reply": "2025-03-11T00:11:44.487654Z"
    },
    "papermill": {
     "duration": 0.025637,
     "end_time": "2025-03-11T00:11:44.489319",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.463682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "verifying_prompt: str = (\n",
    "    \"\"\"\n",
    "This is the problem statement.\n",
    "\n",
    "{problem_statement}\n",
    "\n",
    "These are the files that is thought to be relevant, which may not be complete.\n",
    "\n",
    "{file_content_string}\n",
    "\n",
    "This is the proposed patch to fix the problem.\n",
    "\n",
    "{patch_string}\n",
    "\n",
    "Evaluate whether the patch works\n",
    "- The patch fully fixes the problem described in the problem statement.\n",
    "- The patch does not cause side effects and make any other tests fail.\n",
    "\n",
    "End your response with exactly either of\n",
    "- <label>Yes</label>, this fixes the problem.\n",
    "- <label>No</label>, this does not fix the problem.\n",
    "\n",
    "Reminder\n",
    "- Only evaluate, do not provide suggestion on how to fix.\n",
    "- Remember to write exactly either of <label>Yes</label> or <label>No</label> in the last line\n",
    "\"\"\".strip()\n",
    ")\n",
    "\n",
    "\n",
    "def is_valid_patch_format(patch_string: str) -> bool:\n",
    "    \"\"\"\n",
    "    A quick check to confirm if a patch could be valid.\n",
    "    \"\"\"\n",
    "    if not(isinstance(patch_string, str)):\n",
    "        return False\n",
    "    try:\n",
    "        patch_set = unidiff.PatchSet(patch_string)\n",
    "        if len(patch_set) == 0:\n",
    "            return False\n",
    "    except Exception:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def patch_dry_run_succeeds(patch_string: str, repo_path: str = REPO_PATH, timeout: int = 60) -> bool:\n",
    "    \"\"\"\n",
    "    A robust check if the patch will proceed without any errors.\n",
    "    Should be run after `is_valid_patch_format()`: the patch\n",
    "    command can hang if the inputs are sufficiently invalid.\n",
    "\n",
    "    Args:\n",
    "        patch_path: Path to a file containing the patch.\n",
    "        repo_path: Path to the directory to be patched.\n",
    "        timeout: Number of seconds before the dry run will be cancelled.\n",
    "    \"\"\"\n",
    "    with open(\"patch.txt\", \"w\") as f:\n",
    "        f.write(patch_string)\n",
    "    patch_path = \"/kaggle/working/patch.txt\"\n",
    "\n",
    "    cmd = f\"patch --quiet --dry-run -p1 -i {patch_path} -d {repo_path}\"\n",
    "    try:\n",
    "        subprocess.run(cmd, shell=True, check=True, timeout=timeout)\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_verification(\n",
    "    problem_statement: str,\n",
    "    file_content_strings: List[str],\n",
    "    patch_strings: List[Optional[str]],\n",
    "    repo_path: str,\n",
    ") -> Tuple[List[List[str]], List[List[bool]]]:\n",
    "    assert len(file_content_strings) == len(patch_strings)\n",
    "    sampling_params: SamplingParams = SamplingParams(\n",
    "        temperature=0.3,  # randomness of the sampling\n",
    "        min_p=0.01,\n",
    "        skip_special_tokens=True,  # Whether to skip special tokens in the output\n",
    "        max_tokens=MAX_TOKENS,\n",
    "    )\n",
    "\n",
    "    inference_idx_to_input_idx: list[int] = [\n",
    "        input_idx\n",
    "        for _ in range(VALIDATION_COPY_COUNT)\n",
    "        for input_idx, patch_string in enumerate(patch_strings)\n",
    "        if patch_string is not None and is_valid_patch_format(patch_string) # and patch_dry_run_succeeds(patch_string, repo_path)\n",
    "    ]\n",
    "    print(inference_idx_to_input_idx)\n",
    "\n",
    "    list_of_messages: List[List[Dict[str, str]]] = [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": verifying_prompt.format(\n",
    "                    problem_statement=problem_statement[:20_000],\n",
    "                    file_content_string=file_content_strings[input_idx][:30_000],\n",
    "                    patch_string=patch_strings[input_idx],\n",
    "                ),\n",
    "            },\n",
    "        ]\n",
    "        for input_idx in inference_idx_to_input_idx\n",
    "    ]\n",
    "\n",
    "    prompt_texts: List[str] = [\n",
    "        (\n",
    "            tokenizer.apply_chat_template(\n",
    "                conversation=messages, tokenize=False, add_generation_prompt=True\n",
    "            )  # type: ignore\n",
    "        )\n",
    "        + \"<think>\\n\"\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "    # print(prompt_texts)\n",
    "\n",
    "    print(\"get_verification\", [count_tokens(text) for text in prompt_texts])\n",
    "    request_outputs: list[RequestOutput] = llm.generate(\n",
    "        prompt_texts, sampling_params=sampling_params\n",
    "    )\n",
    "    response_texts: List[str] = [\n",
    "        request_output.outputs[0].text for request_output in request_outputs\n",
    "    ]\n",
    "    print(\"get_verification\", [count_tokens(text) for text in response_texts])\n",
    "\n",
    "    completion_texts = [\n",
    "        prompt_text + response_text\n",
    "        for prompt_text, response_text in zip(prompt_texts, response_texts)\n",
    "    ]\n",
    "    judgments_flattened: List[bool] = [\n",
    "        \"<label>Yes</label>\" in response_text for response_text in response_texts\n",
    "    ]\n",
    "    print(judgments_flattened)\n",
    "\n",
    "    judgments_aggregated: List[List[bool]] = [[] for _ in file_content_strings]\n",
    "    completion_text_aggregated: List[List[str]] = [[] for _ in patch_strings]\n",
    "    for inference_idx, (completion_text, judgement) in enumerate(\n",
    "        zip(completion_texts, judgments_flattened)\n",
    "    ):\n",
    "        input_idx = inference_idx_to_input_idx[inference_idx]\n",
    "        completion_text_aggregated[input_idx].append(completion_text)\n",
    "        judgments_aggregated[input_idx].append(judgement)\n",
    "    print(judgments_aggregated)\n",
    "\n",
    "    return completion_text_aggregated, judgments_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a919dbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:44.515483Z",
     "iopub.status.busy": "2025-03-11T00:11:44.515245Z",
     "iopub.status.idle": "2025-03-11T00:11:44.525337Z",
     "shell.execute_reply": "2025-03-11T00:11:44.524724Z"
    },
    "papermill": {
     "duration": 0.024211,
     "end_time": "2025-03-11T00:11:44.526396",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.502185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unidiff\n",
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "def calculate_patch_score(\n",
    "    patch_string: Optional[str],\n",
    "    judgments: List[bool],\n",
    "    repo_path: str,\n",
    "    validation_copy_count: int = VALIDATION_COPY_COUNT\n",
    ") -> float:\n",
    "\n",
    "    score = 0.0\n",
    "\n",
    "    if patch_string is None:\n",
    "        return -5.0  \n",
    "\n",
    "    if not is_valid_patch_format(patch_string):\n",
    "        return -4.0  \n",
    "\n",
    "    if not patch_dry_run_succeeds(patch_string, repo_path):\n",
    "        return -3.0  \n",
    "\n",
    "    judgment_weight = 5.0  \n",
    "    valid_format_bonus = 0.5 \n",
    "    dry_run_bonus = 0.5  \n",
    "    size_penalty_factor = 1.0  \n",
    "    file_count_penalty = 0.0  \n",
    "\n",
    "    if judgments.count(True) == 0:\n",
    "        return -200.0  \n",
    "\n",
    "    score += judgments.count(True) ** 2 * judgment_weight\n",
    "\n",
    "    score -= valid_format_bonus\n",
    "    score -= dry_run_bonus\n",
    "\n",
    "    score -= patch_lines_penalty_exponential_aggressive_extreme(patch_string, size_penalty_factor) \n",
    "\n",
    "    patch_set = unidiff.PatchSet(patch_string)\n",
    "    num_files_changed = len(patch_set)\n",
    "    score -= num_files_changed * file_count_penalty\n",
    "\n",
    "    return score\n",
    "\n",
    "def patch_lines_penalty_exponential_aggressive_extreme(patch_string: str, penalty_factor: float) -> float:\n",
    "    patch_lines = patch_string.strip().count('\\n') + 1\n",
    "    if patch_lines <= 15: \n",
    "        return patch_lines * penalty_factor * 0.1 \n",
    "    return (np.exp(patch_lines / 10) - 1) * penalty_factor\n",
    "\n",
    "def choose_patch_string_optimized(\n",
    "    patch_strings: list[Optional[str]], judgments_aggregated: List[List[bool]], repo_path: str, correction_threshold_percentile: float = 99.0, validation_copy_count: int = VALIDATION_COPY_COUNT, confidence_margin_ratio: float = 0.7, min_yes_votes: int = 2 # Thêm min_yes_votes\n",
    ") -> tuple[list[int], Optional[str]]:\n",
    "    best_score = -float('inf')\n",
    "    best_patch_string = None\n",
    "    scores = []\n",
    "\n",
    "    all_patches_negative_score: bool = True\n",
    "    correction_threshold = 0.0 \n",
    "\n",
    "    for patch_string, judgments in zip(patch_strings, judgments_aggregated):\n",
    "        score = calculate_patch_score(patch_string, judgments, repo_path, validation_copy_count)\n",
    "        scores.append(score)\n",
    "\n",
    "        if score >= correction_threshold:\n",
    "            all_patches_negative_score = False\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_patch_string = patch_string\n",
    "\n",
    "    adaptive_threshold = np.percentile(scores, correction_threshold_percentile)\n",
    "\n",
    "    if scores:\n",
    "        max_score_excluding_best = np.max([s for s in scores if s != best_score] or [-float('inf')])\n",
    "        confidence_margin = best_score - max_score_excluding_best\n",
    "        required_margin = confidence_margin_ratio * abs(best_score)\n",
    "    else:\n",
    "        confidence_margin = -float('inf')\n",
    "        required_margin = 0\n",
    "\n",
    "    if all_patches_negative_score or best_score < adaptive_threshold or confidence_margin < required_margin or sum(judgments) < min_yes_votes: \n",
    "        return scores, None\n",
    "    else:\n",
    "        return scores, best_patch_string\n",
    "\n",
    "\n",
    "def ensemble_patch_selection(patch_strings_list, judgments_aggregated_list):\n",
    "    best_patch = None\n",
    "    max_yes_votes = -1\n",
    "\n",
    "    for patch_string, judgments in zip(patch_strings_list, judgments_aggregated_list):\n",
    "        yes_votes = judgments.count(True)\n",
    "        if yes_votes > max_yes_votes:\n",
    "            max_yes_votes = yes_votes\n",
    "            best_patch = patch_string\n",
    "\n",
    "    return best_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a121d75",
   "metadata": {
    "papermill": {
     "duration": 0.012289,
     "end_time": "2025-03-11T00:11:44.551360",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.539071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "219ab3db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:44.577278Z",
     "iopub.status.busy": "2025-03-11T00:11:44.577037Z",
     "iopub.status.idle": "2025-03-11T00:11:44.583430Z",
     "shell.execute_reply": "2025-03-11T00:11:44.582841Z"
    },
    "papermill": {
     "duration": 0.020443,
     "end_time": "2025-03-11T00:11:44.584433",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.563990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict function\n",
    "def predict_inner(problem_statement: str, directory: str) -> Optional[str]:\n",
    "    directory_string = stringify_directory(directory)\n",
    "\n",
    "    selection_completion_texts, file_queries = get_selection_query(\n",
    "        directory_string, problem_statement\n",
    "    )\n",
    "\n",
    "    file_content_strings: List[str] = [\n",
    "        fetch_file_contents(file_query) for file_query in file_queries\n",
    "    ]\n",
    "\n",
    "    patch_completion_texts, patch_strings = get_patch_string(\n",
    "        problem_statement, file_content_strings\n",
    "    )\n",
    "\n",
    "    verification_completion_texts_aggregated, judgments_aggregated = get_verification(\n",
    "        problem_statement, file_content_strings, patch_strings, directory\n",
    "    )\n",
    "\n",
    "    scores, patch_string = choose_patch_string_optimized(\n",
    "        patch_strings, judgments_aggregated, directory, VALIDATION_COPY_COUNT\n",
    "    )\n",
    "\n",
    "    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "        data = {\n",
    "            \"problem_statement\": [problem_statement] * len(file_queries),\n",
    "            \"selection_completion_text\": selection_completion_texts,\n",
    "            \"selection_completion_length\": [\n",
    "                count_tokens(completion_text)\n",
    "                for completion_text in selection_completion_texts\n",
    "            ],\n",
    "            \"file_query\": file_queries,\n",
    "            \"file_content_string\": file_content_strings,\n",
    "            \"patch_completion_text\": patch_completion_texts,\n",
    "            \"patch_completion_length\": [\n",
    "                count_tokens(completion_text)\n",
    "                for completion_text in patch_completion_texts\n",
    "            ],\n",
    "            \"patch_string\": patch_strings, \n",
    "        }\n",
    "\n",
    "        for copy_idx in range(VALIDATION_COPY_COUNT):\n",
    "            data[f\"verification_completion_text_{copy_idx}\"] = [\n",
    "                completion_texts[copy_idx] if completion_texts else None\n",
    "                for completion_texts in verification_completion_texts_aggregated\n",
    "            ]\n",
    "            data[f\"verification_completion_length_{copy_idx}\"] = [\n",
    "                count_tokens(completion_texts[copy_idx]) if completion_texts else None\n",
    "                for completion_texts in verification_completion_texts_aggregated\n",
    "            ]\n",
    "            data[f\"judgment_{copy_idx}\"] = [\n",
    "                judgments[copy_idx] if judgments else None\n",
    "                for judgments in judgments_aggregated\n",
    "            ]\n",
    "\n",
    "        data[\"judgment_count_true\"] = [judgments.count(True) for judgments in judgments_aggregated] \n",
    "        data[\"score\"] = scores\n",
    "\n",
    "        elapsed_time_int = int(time.time() - start_time)  \n",
    "\n",
    "        pd.DataFrame(data).to_csv(  \n",
    "            f\"{str(elapsed_time_int).zfill(5)}.csv\", index=False  \n",
    "        )\n",
    "\n",
    "    return patch_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae70913a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:44.610158Z",
     "iopub.status.busy": "2025-03-11T00:11:44.609922Z",
     "iopub.status.idle": "2025-03-11T00:11:44.615015Z",
     "shell.execute_reply": "2025-03-11T00:11:44.614410Z"
    },
    "papermill": {
     "duration": 0.019146,
     "end_time": "2025-03-11T00:11:44.616060",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.596914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import Optional, List\n",
    "\n",
    "skip_prediction: bool = False\n",
    "\n",
    "\n",
    "def predict(\n",
    "    problem_statement: str,\n",
    "    repo_archive: io.BytesIO,\n",
    "    pip_packages_archive: io.BytesIO,\n",
    "    env_setup_cmds_templates: List[str],\n",
    ") -> Optional[str]:\n",
    "    \"\"\"Replace this function with your inference code.\n",
    "    Args:\n",
    "        problem_statement: The text of the git issue.\n",
    "        repo_archive: A BytesIO buffer path with a .tar containing the codebase that must be patched. The gateway will make this directory available immediately before this function runs.\n",
    "    \"\"\"\n",
    "    global skip_prediction\n",
    "    if skip_prediction:\n",
    "        return None\n",
    "\n",
    "    with open(\"repo_archive.tar\", \"wb\") as f:\n",
    "        f.write(repo_archive.read())\n",
    "    repo_path: str = REPO_PATH\n",
    "    if os.path.exists(repo_path):\n",
    "        shutil.rmtree(repo_path)\n",
    "    shutil.unpack_archive(\"repo_archive.tar\", extract_dir=repo_path)\n",
    "    os.remove(\"repo_archive.tar\")\n",
    "\n",
    "    patch_string: Optional[str] = None\n",
    "    patch_string = predict_inner(\n",
    "        problem_statement=problem_statement, directory=repo_path\n",
    "    )\n",
    "    shutil.rmtree(repo_path)\n",
    "\n",
    "    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "        skip_prediction = True\n",
    "\n",
    "    print(\"submitted patch_string\")\n",
    "    print(patch_string)\n",
    "\n",
    "    if patch_string is None:\n",
    "        return None\n",
    "\n",
    "    return patch_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caa6711",
   "metadata": {
    "papermill": {
     "duration": 0.012406,
     "end_time": "2025-03-11T00:11:44.642129",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.629723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get predict data without server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "296fcf8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:44.668213Z",
     "iopub.status.busy": "2025-03-11T00:11:44.667968Z",
     "iopub.status.idle": "2025-03-11T00:11:48.127329Z",
     "shell.execute_reply": "2025-03-11T00:11:48.126528Z"
    },
    "papermill": {
     "duration": 3.474174,
     "end_time": "2025-03-11T00:11:48.128837",
     "exception": false,
     "start_time": "2025-03-11T00:11:44.654663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# !mkdir -p /kaggle/tmp/konwinski-prize-alt\n",
    "os.makedirs(\"/kaggle/tmp/konwinski-prize-alt\", exist_ok=True)\n",
    "\n",
    "# !unzip -q -o /kaggle/input/konwinski-prize/data.a_zip -d /kaggle/tmp/konwinski-prize-alt/ 2>/dev/null || true\n",
    "try:\n",
    "    with zipfile.ZipFile(\"/kaggle/input/konwinski-prize/data.a_zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"/kaggle/tmp/konwinski-prize-alt/\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80e4415b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:48.155398Z",
     "iopub.status.busy": "2025-03-11T00:11:48.155113Z",
     "iopub.status.idle": "2025-03-11T00:11:48.159813Z",
     "shell.execute_reply": "2025-03-11T00:11:48.159172Z"
    },
    "papermill": {
     "duration": 0.01902,
     "end_time": "2025-03-11T00:11:48.160869",
     "exception": false,
     "start_time": "2025-03-11T00:11:48.141849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_problem(problem_index: int) -> Tuple[str, str, io.BytesIO]:\n",
    "    df = pd.read_parquet(\"/kaggle/tmp/konwinski-prize-alt/data/data.parquet\")\n",
    "\n",
    "    problem_statement: str = df[\"problem_statement\"][problem_index]\n",
    "    repo_path: str = (\n",
    "        f\"/kaggle/tmp/konwinski-prize-alt/data/repos/repo__{df['instance_id'][problem_index]}\"\n",
    "    )\n",
    "\n",
    "    import shutil\n",
    "    import tempfile\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        shutil.make_archive(os.path.join(tmpdir, \"a_repo\"), \"tar\", repo_path)\n",
    "        with open(os.path.join(tmpdir, \"a_repo.tar\"), \"rb\") as f:\n",
    "            repo_archive = io.BytesIO(f.read())\n",
    "\n",
    "    return problem_statement, repo_path, repo_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23a817e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:48.186867Z",
     "iopub.status.busy": "2025-03-11T00:11:48.186592Z",
     "iopub.status.idle": "2025-03-11T00:11:48.190230Z",
     "shell.execute_reply": "2025-03-11T00:11:48.189622Z"
    },
    "papermill": {
     "duration": 0.017783,
     "end_time": "2025-03-11T00:11:48.191237",
     "exception": false,
     "start_time": "2025-03-11T00:11:48.173454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo_problem_index: int = 0\n",
    "\n",
    "if os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not os.getenv(\n",
    "    \"KAGGLE_IS_COMPETITION_RERUN\"\n",
    "):\n",
    "    problem_statement, repo_path, repo_archive = get_problem(\n",
    "        problem_index=demo_problem_index\n",
    "    )\n",
    "\n",
    "    print(repo_path)\n",
    "    print(problem_statement)\n",
    "    print(len(list(repo_archive)))\n",
    "    print(len(list(repo_archive)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c1a24f8",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:48.217248Z",
     "iopub.status.busy": "2025-03-11T00:11:48.217000Z",
     "iopub.status.idle": "2025-03-11T00:11:48.220356Z",
     "shell.execute_reply": "2025-03-11T00:11:48.219750Z"
    },
    "papermill": {
     "duration": 0.01749,
     "end_time": "2025-03-11T00:11:48.221381",
     "exception": false,
     "start_time": "2025-03-11T00:11:48.203891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not os.getenv(\n",
    "    \"KAGGLE_IS_COMPETITION_RERUN\"\n",
    "):\n",
    "    skip_prediction = False\n",
    "    problem_statement, repo_path, repo_archive = get_problem(\n",
    "        problem_index=demo_problem_index\n",
    "    )\n",
    "    patch_string = predict(problem_statement, repo_archive, io.BytesIO(), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bca91f0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:48.247378Z",
     "iopub.status.busy": "2025-03-11T00:11:48.247137Z",
     "iopub.status.idle": "2025-03-11T00:11:48.251162Z",
     "shell.execute_reply": "2025-03-11T00:11:48.250564Z"
    },
    "papermill": {
     "duration": 0.018097,
     "end_time": "2025-03-11T00:11:48.252181",
     "exception": false,
     "start_time": "2025-03-11T00:11:48.234084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\"\n",
    "    and not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n",
    "    and patch_string is not None\n",
    "):\n",
    "    import polars as pl\n",
    "\n",
    "    df = pl.read_parquet(\"/kaggle/tmp/konwinski-prize-alt/data/data.parquet\")\n",
    "\n",
    "    import kaggle_evaluation.konwinski_prize_gateway\n",
    "\n",
    "    k_prize_gateway = kaggle_evaluation.konwinski_prize_gateway.KPrizeGateway()\n",
    "    k_prize_gateway.unpack_data_paths()\n",
    "\n",
    "    results = k_prize_gateway._evaluate_instance(\n",
    "        instance=df.row(demo_problem_index, named=True),\n",
    "        patch=patch_string,\n",
    "    )\n",
    "\n",
    "    from collections import Counter\n",
    "    print(\n",
    "        demo_problem_index, Counter(result.unit_test_outcome for result in results[1:])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c8b5521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:48.278299Z",
     "iopub.status.busy": "2025-03-11T00:11:48.278048Z",
     "iopub.status.idle": "2025-03-11T00:11:48.281598Z",
     "shell.execute_reply": "2025-03-11T00:11:48.280971Z"
    },
    "papermill": {
     "duration": 0.017707,
     "end_time": "2025-03-11T00:11:48.282614",
     "exception": false,
     "start_time": "2025-03-11T00:11:48.264907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\"\n",
    "    and not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n",
    "    and patch_string is not None\n",
    "):\n",
    "    from kaggle_evaluation.konwinski_prize_gateway import UnitTestOutcome\n",
    "\n",
    "    for result in results[1:]:\n",
    "        if result.unit_test_outcome != UnitTestOutcome.PASSED:\n",
    "            print(result.test_name)\n",
    "            print(result.fail_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6014ae",
   "metadata": {
    "papermill": {
     "duration": 0.01253,
     "end_time": "2025-03-11T00:11:48.307904",
     "exception": false,
     "start_time": "2025-03-11T00:11:48.295374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation with inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fd91a5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:48.333771Z",
     "iopub.status.busy": "2025-03-11T00:11:48.333526Z",
     "iopub.status.idle": "2025-03-11T00:11:48.336446Z",
     "shell.execute_reply": "2025-03-11T00:11:48.335842Z"
    },
    "papermill": {
     "duration": 0.016986,
     "end_time": "2025-03-11T00:11:48.337510",
     "exception": false,
     "start_time": "2025-03-11T00:11:48.320524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_prediction = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67f38d9d",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-03-11T00:11:48.363634Z",
     "iopub.status.busy": "2025-03-11T00:11:48.363391Z",
     "iopub.status.idle": "2025-03-11T00:19:13.757916Z",
     "shell.execute_reply": "2025-03-11T00:19:13.757049Z"
    },
    "papermill": {
     "duration": 445.409247,
     "end_time": "2025-03-11T00:19:13.759484",
     "exception": false,
     "start_time": "2025-03-11T00:11:48.350237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing uv installation found. Skipping uv installation.\n",
      "Installing Python 3.11...\n",
      "get_selection_query [4203, 4203, 4203, 4203, 4203, 4203, 4203]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 7/7 [02:06<00:00, 18.07s/it, est. speed input: 232.59 toks/s, output: 111.21 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_selection_query [2964, 1621, 2341, 1906, 1746, 1979, 1500]\n",
      "get_patch_string [1011, 10125, 1019, 1533, 1827, 2511, 1519]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 7/7 [03:55<00:00, 33.62s/it, est. speed input: 83.04 toks/s, output: 113.65 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_patch_string [3648, 3375, 4096, 3781, 4096, 3651, 4096]\n",
      "[1, 5, 1, 5, 1, 5]\n",
      "get_verification [10225, 2637, 10225, 2637, 10225, 2637]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [01:10<00:00, 11.74s/it, est. speed input: 547.57 toks/s, output: 47.10 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_verification [732, 576, 488, 467, 468, 582]\n",
      "[True, True, True, True, True, True]\n",
      "[[], [True, True, True], [], [], [], [True, True, True], []]\n",
      "submitted patch_string\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inference_server = (\n",
    "    kaggle_evaluation.konwinski_prize_inference_server.KPrizeInferenceServer(\n",
    "        get_number_of_instances, predict\n",
    "    )\n",
    ")\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            \"/kaggle/input/konwinski-prize/\",  # Path to the entire competition dataset\n",
    "            \"/kaggle/tmp/konwinski-prize/\",  # Path to a scratch directory for unpacking data.a_zip.\n",
    "        )  # type: ignore\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a8c105e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T00:19:13.789918Z",
     "iopub.status.busy": "2025-03-11T00:19:13.789601Z",
     "iopub.status.idle": "2025-03-11T00:19:13.830734Z",
     "shell.execute_reply": "2025-03-11T00:19:13.830023Z"
    },
    "papermill": {
     "duration": 0.057588,
     "end_time": "2025-03-11T00:19:13.831874",
     "exception": false,
     "start_time": "2025-03-11T00:19:13.774286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.309916 [(4, 26, 41)] \n",
      "\n",
      "-0.028260 [(2, 4, 65)] \n",
      "\n",
      "-0.126839 [(3, 12, 56)] \n",
      "\n",
      "-0.112756 [(3, 11, 57)] \n",
      "\n",
      "-0.140919 [(4, 14, 53)] \n",
      "\n",
      "-0.112753 [(4, 12, 55)] \n",
      "\n",
      "-0.042346 [(1, 4, 66)] \n",
      "\n",
      "-0.014180 [(1, 2, 68)] \n",
      "\n",
      "1.000000 [(71, 0, 0)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_score(\n",
    "    n_correct: int,\n",
    "    n_wrong: int,\n",
    "    n_skipped: int,\n",
    "    incorrect_score=-1,\n",
    "    skip_score=-10**-4\n",
    ") -> str:\n",
    "    score = (n_correct + n_wrong * incorrect_score + n_skipped * skip_score) / (n_correct + n_skipped + n_wrong)\n",
    "    if n_correct == 0:\n",
    "        score = incorrect_score\n",
    "    return f\"{score:+.20f}\"[:1+1+1+6].lstrip(\"+\")\n",
    "\n",
    "\n",
    "def calculate_results(score: str) -> list[tuple[int, int, int]]:\n",
    "    assert type(score) == str\n",
    "    possible_results = []\n",
    "    for n_correct in range(72):\n",
    "        for n_wrong in range(72 - n_correct):\n",
    "            n_skipped = 71 - n_correct - n_wrong\n",
    "            if score == calculate_score(n_correct, n_wrong, n_skipped):\n",
    "                 possible_results.append((n_correct, n_wrong, n_skipped))\n",
    "    return possible_results\n",
    "\n",
    "\n",
    "for score in [\n",
    "    \"-0.309916\",\n",
    "    \"-0.028260\",\n",
    "    \"-0.126839\",\n",
    "    \"-0.112756\",\n",
    "    \"-0.140919\",\n",
    "    \"-0.112753\",\n",
    "    \"-0.042346\",\n",
    "    \"-0.014180\",\n",
    "    \"1.000000\",\n",
    "]:\n",
    "    print(score, calculate_results(score), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be51412",
   "metadata": {
    "papermill": {
     "duration": 0.014259,
     "end_time": "2025-03-11T00:19:13.860729",
     "exception": false,
     "start_time": "2025-03-11T00:19:13.846470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192d602",
   "metadata": {
    "papermill": {
     "duration": 0.014124,
     "end_time": "2025-03-11T00:19:13.889108",
     "exception": false,
     "start_time": "2025-03-11T00:19:13.874984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dbbdb0",
   "metadata": {
    "papermill": {
     "duration": 0.054555,
     "end_time": "2025-03-11T00:19:13.957944",
     "exception": false,
     "start_time": "2025-03-11T00:19:13.903389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de6588",
   "metadata": {
    "papermill": {
     "duration": 0.014284,
     "end_time": "2025-03-11T00:19:13.986677",
     "exception": false,
     "start_time": "2025-03-11T00:19:13.972393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11281725,
     "sourceId": 84795,
     "sourceType": "competition"
    },
    {
     "sourceId": 221096520,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 224053,
     "modelInstanceId": 236741,
     "sourceId": 276458,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 807.11668,
   "end_time": "2025-03-11T00:19:17.721031",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-11T00:05:50.604351",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "18e339d886034906a3e7a227e4d62c3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "306360ab30a44148b8b70a1f84938451": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "485ca1f8eb6e43b5944c051226536729": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "48a32c65b6b24875ad74f6d71fd383f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4c83db32fb734961a9690d9b5079c246": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_306360ab30a44148b8b70a1f84938451",
       "placeholder": "​",
       "style": "IPY_MODEL_48a32c65b6b24875ad74f6d71fd383f0",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:38&lt;00:00, 34.13s/it]\n"
      }
     },
     "5e37fc1145094175b74260d6fef00bdb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f125d32fc324cec908762a34b78357d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5e37fc1145094175b74260d6fef00bdb",
       "max": 5,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_18e339d886034906a3e7a227e4d62c3c",
       "tabbable": null,
       "tooltip": null,
       "value": 5
      }
     },
     "8ae15c8147e04fadae938c71117a24da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a8749ac89e53475f843b2dae20109196": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b24206d3afa448e6b5b8159a5301c55f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8ae15c8147e04fadae938c71117a24da",
       "placeholder": "​",
       "style": "IPY_MODEL_a8749ac89e53475f843b2dae20109196",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "d3f311b5e5514069857025940bef314a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b24206d3afa448e6b5b8159a5301c55f",
        "IPY_MODEL_6f125d32fc324cec908762a34b78357d",
        "IPY_MODEL_4c83db32fb734961a9690d9b5079c246"
       ],
       "layout": "IPY_MODEL_485ca1f8eb6e43b5944c051226536729",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
